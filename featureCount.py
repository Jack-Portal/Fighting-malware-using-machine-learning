# -*- coding: utf-8 -*-
"""
    PROJECT: Fighting malware using machine learning.
    
    This file contains all the functions used to get the features from a file
    or from multiple files (from lists of integers representing the opcodes).
    @author: Jacques-Antoine Portal
    """

import os
import pickle
import makeSubset as MKS
import hashArray as HA
import progressbar
import scipy.sparse as sp
import time
from sklearn.preprocessing import normalize



def getOpcodesIndex(file = "OPCODEDICT.pkl"):
    '''
        This function takes as argument a file name containing the dictionary of opcodes
        made when counting all the opcodes and returns a dictionary taking integers as
        keys and opcodes strings as value.
        '''
    ofile = open(file, "rb")
    OPCODESDICT = pickle.load(ofile)
    ofile.close()
    OPCODES = [0 for i in range(len(OPCODESDICT.keys()))]
    for e in OPCODESDICT:
        OPCODES[OPCODESDICT[e]] = e
    return OPCODES

OPCODES = getOpcodesIndex()


def loadPklFile(fileName, folderName = './'):
    '''
        Function that loads .asm.pkl file
        '''
    file = open(os.path.join(folderName, fileName + ".asm.pkl" ), 'rb')
    opcodes = pickle.load(file)
    file.close()
    return opcodes


def dumpPklFile(toDump, fileName, folderName = None):
    '''
        Function that dumps an object to a .pkl file
        '''
    if folderName is not None:
        output = open(os.path.join(folderName, fileName), 'wb')
        pickle.dump(toDump, output, -1)
        output.close()
    else:
        output = open(fileName, 'wb')
        pickle.dump(toDump, output, -1)
        output.close()

def mkFeatureI(listOfInt):
    '''
        returns a tuple of all the items in the list of int.
        '''
    return tuple(listOfInt)

def hashNgram(Ngram, dictOfNgram, p2 = 18):
    '''
        Thi function takes an Ngram (tuple of n integetrs) as an argument,
        a dictionary of Ngram and the size of hash numbers corresponding to the
        number of features to return the hashed feature as well as the updated
        dictionary of Ngrams.
        '''
    h = HA.getHashN(Ngram, p2)
    if h not in dictOfNgram.keys():
        dictOfNgram[h] = {Ngram:1}
    else:
        if Ngram not in dictOfNgram[h].keys():
            dictOfNgram[h][Ngram] = 1
        else:
            dictOfNgram[h][Ngram] += 1
    return h, dictOfNgram

def hashNgramFast(Ngram, p2 = 18):
    '''
        This function returns a hash number corresponding to the Ngram it has been
        given as an argument. (this hash number is limited in size by the second argument
        p2).
        '''
    return HA.getHashN(Ngram, p2)

def getAllNgrams(maxN, listOfInt, dictOfNgram = {}, p2 = 18):
    '''
        This function takes a list of opcodes (in the form of a list of int) and returns
        a list of all the Ngrams (in order) in te file up to length N.
        This function should only be used if keeping track of the Ngrams and Skip grams
        is needed, otherwise getAllNgramsFast() should be used.
        '''
    listOfNgrams = []
    for N in range(1, maxN + 1):
        listOfNgramTemp = []
        for i in range(len(listOfInt)-N):
            h, dictOfNgram = hashNgram(tuple(listOfInt[i:i+N]), dictOfNgram, p2)
            listOfNgramTemp.append(h)
        listOfNgrams.append(listOfNgramTemp)
    return listOfNgrams, dictOfNgram

def getAllNgramsFast(maxN, listOfInt, p2 = 18):
    '''
        This function takes a list of opcodes (in the form of a list of int) and returns
        a list of all the Ngrams (in order) in te file up to length N
        '''
    listOfNgrams = []
    for N in range(1, maxN + 1):
        listOfNgramTemp = []
        for i in range(len(listOfInt)-N):
            h = hashNgramFast(tuple(listOfInt[i:i+N]), p2)
            listOfNgramTemp.append(h)
        listOfNgrams.append(listOfNgramTemp)
    return listOfNgrams


def getAllSkipGrams(skipDistance, listOfNgrams, dictOfNgram = {}, p2 = 18):
    '''
        This function takes a list of Ngrams (in the form of a list of list of int) and returns
        a list of all the skipgrams (in order) in te file up to length skipDistance.
        
        This function should only be used if keeping track of the Ngrams and Skip grams
        is needed, otherwise getAllSkipGramsFast() should be used.
        '''
    listOfSkip = []
    for listIndex in range(len(listOfNgrams)):
        if listIndex+1 <= skipDistance:
            l = listOfNgrams[listIndex]
            listOfSkipTemp = [[] for i in range(len(listOfNgrams))]
            for i in range(len(l)-skipDistance):
                for otherListIndex in range(len(listOfNgrams)):
                    otherList = listOfNgrams[otherListIndex]
                    if i < len(otherList)-skipDistance:
                        skipGram = tuple([l[i]]+[otherList[i+skipDistance]]+[skipDistance])
                        h, dictOfNgram = hashNgram(skipGram, dictOfNgram, p2)
                        listOfSkipTemp[otherListIndex].append(h)
            listOfSkip.append(listOfSkipTemp)
    return listOfSkip, dictOfNgram

def getAllSkipGramsFast(skipDistance, listOfNgrams, p2 = 18):
    '''
        This function takes a list of Ngrams (in the form of a list of list of int) and returns
        a list of all the skipgrams (in order) in te file up to length skipDistance
        '''
    listOfSkip = []
    for listIndex in range(len(listOfNgrams)):
        if listIndex+1 <= skipDistance:
            l = listOfNgrams[listIndex]
            listOfSkipTemp = [[] for i in range(len(listOfNgrams))]
            for i in range(len(l)-skipDistance):
                for otherListIndex in range(len(listOfNgrams)):
                    otherList = listOfNgrams[otherListIndex]
                    if i < len(otherList)-skipDistance:
                        skipGram = tuple([l[i]]+[otherList[i+skipDistance]]+[skipDistance])
                        h = hashNgramFast(skipGram, p2)
                        listOfSkipTemp[otherListIndex].append(h)
            listOfSkip.append(listOfSkipTemp)
    return listOfSkip

def getAllSkipGramsInRange(skipDistances, listOfNgrams, dictOfNgram = {}, p2 = 18):
    '''
        This function returns a list of list of list of skip gram with distances up to skipDistances.
        This function should only be used if keeping track of the Ngrams and Skip grams
        is needed, otherwise getAllSkipGramsInRangeFast() should be used.
        '''
    listsOfSkip = []
    for i in range(1, skipDistances+1):
        listOfSkip, dictOfNgram = getAllSkipGrams(i, listOfNgrams, dictOfNgram, p2)
        listsOfSkip += [listOfSkip]
    return listsOfSkip, dictOfNgram

def getAllSkipGramsInRangeFast(skipDistances, listOfNgrams, p2 = 18):
    '''
        This function returns a list of list of list of skip gram with distances up to skipDistances.
        '''
    listsOfSkip = []
    for i in range(1, skipDistances+1):
        listOfSkip = getAllSkipGramsFast(i, listOfNgrams, p2)
        listsOfSkip += [listOfSkip]
    return listsOfSkip


def countNGramDict(NGramList, powerOf2):
    '''
        This function counts the number of time a single Ngram is used in the dictionary
        of Ngrams provided as argument. it then returns a dictionary containing all the
        counts for each Ngram.
        '''
    featureCount = {}
    for firstNgramLenList in NGramList:
        for skipDistanceList in firstNgramLenList:
            for ngram in skipDistanceList:
                feature = HA.getHashN(ngram, powerOf2)
                if feature not in featureCount.keys():
                    featureCount[feature] = 1
                else:
                    featureCount[feature] += 1
    return featureCount

def dictionaryToSparse(fileToFeature, subset, powerOf2):
    '''
        This function takes as argument the list of all the count dictionary (for every file),
        the subset of files, and the power of 2 used to limit the size of hashes, and
        creates a sparse matrix that can then be used for training a machine learning algorithm.
        '''
    #all the lists needed to create the sparse matrix.
    row = []
    col = []
    data = []
    for r in range(len(subset)):
        for c in fileToFeature[r].keys():
            row += [r]
            col += [c]
            data += [fileToFeature[r][c]]
        fileToFeature[r]= None
    return sp.csr_matrix((data, (row, col)), shape=(len(fileToFeature), pow(2, powerOf2+1)))

def prepareFeatureCountIDict(maxN = 4, maxDistance = 2, subset = None, subsetCount = 100, inputFolder = "OpcodeList/", powerOf2 = 18):
    '''
        This fucntion basically combines all the previous functions to get all the features
        and to keep track of the usefull dictionaries to get ngram and skip grams instead of
        just numbers when analysing the feratures.
        '''
    #initialisation
    toReturn, subset =  (False, subset) if subset is not None else (True, MKS.createSubset(subsetCount))
    fileToFeature = []
    NgramDict = {}
    SkipDict = {}
    for filename in progressbar.ProgressBar()(subset):
        #for filename in subset:
        #gettign all the different lists of Ngrams and Skipgrams
        opcodes = loadPklFile(filename, inputFolder)
        NGramLists, NgramDict = getAllNgrams(maxN, opcodes, NgramDict, powerOf2)
        SkipGramLists, SkipDict = getAllSkipGramsInRange(maxDistance, NGramLists, SkipDict, powerOf2)
        featureCountList = []
        for i in SkipGramLists:
            featureCountList += i
        #counting how many times each of them occur in a given file
        featureCount = countNGramDict(featureCountList, powerOf2)
        fileToFeature += [featureCount]
    #getting a standardised sparse matrix to use for training.
    sparseMatrix = dictionaryToSparse(fileToFeature, subset, powerOf2)
    if toReturn:
        return (subset, sparseMatrix, NgramDict, SkipDict)
    return (sparseMatrix, NgramDict, SkipDict)

def getTrainingFeatures(maxN = 4, maxDistance = 2, subset = None, subsetCount = 100, inputFolder = "OpcodeList/", powerOf2 = 18):
    '''
        This fucntion basically combines all the previous functions to get all the features
        and return them in a sparse matrix.
        '''
    toReturn, subset =  (False, subset) if subset is not None else (True, MKS.createSubset(subsetCount))
    fileToFeature = []
    for filename in subset:
        #for filename in subset:
        #gettign all the different lists of Ngrams and Skipgrams
        opcodes = loadPklFile(filename, inputFolder)
        NGramLists = getAllNgramsFast(maxN, opcodes, powerOf2)
        SkipGramLists = getAllSkipGramsInRangeFast(maxDistance, NGramLists, powerOf2)
        featureCountList = []
        for i in SkipGramLists:
            featureCountList += i
        #counting how many times each of them occur in a given file
        featureCount = countNGramDict(featureCountList, powerOf2)
        fileToFeature += [featureCount]
    #getting a standardised sparse matrix to use for training.
    sparseMatrix = dictionaryToSparse(fileToFeature, subset, powerOf2)
    if toReturn:
        return (subset, sparseMatrix)
    return sparseMatrix

def FeatureCount(maxN = 3, maxDistance = 3, outputFolder = 'trt/', subsetCount = 75, inputDirectory = "OpcodeList/", subset = None, powerOf2 = 18):
    '''
        This function calls the function above and if it detects that the computer is not
        connected to the full set of data, it will use the smaller subset of data found in OpcodeList/.
        It then saves a file to feature array that can be used to train a machine learning classification
        algorithm, a subset file, where all the files / elements of the subset used are stored (in order),
        adn finally the feature array that was used (in order).
        It takes as arguments:
        - outputFolder: a folder in the current dictionary where to store the created files
        - subsetCount: the number of items in a subset if no subset is provided
        - inputDirectory: where the files / lists of opcodes can be found
        - subset: a list of fileNames.
        '''
    if subset is None:
        print("No subset provided; creating one")
        time.sleep(1)
        subset, fileToFeature, FEATUREARRAY, SKIPDICT = prepareFeatureCountIDict(maxN, maxDistance, subsetCount = subsetCount, powerOf2 = powerOf2, inputFolder = inputDirectory)
    #MKS.makeSubsetFolder(SUBSET)
    else :
        print('Subset has been provided')
        time.sleep(1)
        fileToFeature, FEATUREARRAY, SKIPDICT = prepareFeatureCountIDict(maxN, maxDistance, subset)
    dumpPklFile(fileToFeature, "fileToFeature.pkl", outputFolder)
    dumpPklFile(subset, "theSubset.pkl", outputFolder)
    dumpPklFile(FEATUREARRAY, "FeatureArray.pkl", outputFolder)
    dumpPklFile(SKIPDICT, "SKIPDICT.pkl", outputFolder)
    dumpPklFile((maxN, maxDistance, powerOf2), "algorithmConfig.pkl")
    return FEATUREARRAY, SKIPDICT, fileToFeature, subset

def getFeatureMatrix(maxN = 3, maxDistance = 3, outputFolder = 'trt/', subsetCount = 75, inputDirectory = "OpcodeList/", subset = None, powerOf2 = 18):
    '''
        This function calls the function above and if it detects that the computer is not
        connected to the full set of data, it will use the smaller subset of data found in OpcodeList/.
        It then saves a file to feature array that can be used to train a machine learning classification
        algorithm, a subset file, where all the files / elements of the subset used are stored (in order),
        adn finally the feature array that was used (in order).
        It takes as arguments:
        - outputFolder: a folder in the current dictionary where to store the created files
        - subsetCount: the number of items in a subset if no subset is provided
        - inputDirectory: where the files / lists of opcodes can be found
        - subset: a list of fileNames.
        '''
    if subset is None:
        subset, fileToFeature = getTrainingFeatures(maxN, maxDistance, subsetCount = subsetCount, powerOf2 = powerOf2, inputFolder = inputDirectory)
    #MKS.makeSubsetFolder(SUBSET)
    else :
        fileToFeature = getTrainingFeatures(maxN, maxDistance, subset)
    return fileToFeature, subset


def getPredictionFeatures(FileOpcodes, maxN = 3, maxDistance = 3, powerOf2 = 18):
    '''
        This function gets the features form one given list of opcodes and returns them.
        '''
    fileToFeature = []
    NGramLists = getAllNgramsFast(maxN, FileOpcodes, powerOf2)
    SkipGramLists = getAllSkipGramsInRangeFast(maxDistance, NGramLists, powerOf2)
    featureCountList = []
    for i in SkipGramLists:
        featureCountList += i
    featureCount = countNGramDict(featureCountList, powerOf2)
    fileToFeature += [featureCount]
    sparseMatrix = dictionaryToSparse(fileToFeature, [0], powerOf2)
    return sparseMatrix

#these functions are used to get the opcodes / ngrams / skipgrams back using the
#hased or integer version of the opcodes / ngrams / skipgrams.

def d(listOfSkipGramsInRange, index):
    '''
        because listOfSkipGramsInRange does not include the first list for distance
        equals to 1, this takes a distance and returns the corresponding skipGramList
        in a list of SkipGram.
        '''
    return listOfSkipGramsInRange[index-1]

def getSkipDict(listOfSkipGramsInRange, SkipDict = {}, powerOf2=18):
    for s in listOfSkipGramsInRange:
        for t in s:
            h = HA.getHashN(t, powerOf2)
            if h in SkipDict.keys():
                if t not in SkipDict[h]:
                    SkipDict[h] += [t]
            else:
                SkipDict[h] = [t]
    return SkipDict

def intToOpcode1(HashedNumber, dictOfNgram, opcodeArray = OPCODES):
    return tuple([opcodeArray[o] for o in list(max(dictOfNgram[HashedNumber]))])

def intToOpcode2(SkipHashes, dictOfNgram, opcodeArray = OPCODES):
    H1, H2, D = SkipHashes
    return (intToOpcode1(H1, dictOfNgram), intToOpcode1(H2, dictOfNgram), D)

def intToOpcode(Tuple, dictOfNgram = {}, opcodeArray = OPCODES):
    if type(Tuple) is list:
        Tuple = Tuple[0]
    try:
        toReturn = intToOpcode2(Tuple, dictOfNgram, opcodeArray)
    except:
        toReturn = intToOpcode1(Tuple, dictOfNgram, opcodeArray)
    return toReturn

def skipToOpcode(hashNumber, skipDict, dictOfNgram, opcodeArray = OPCODES):
    return intToOpcode(max(skipDict[hashNumber]), dictOfNgram, opcodeArray)

#this is the kind of stuff you could do using these functions:

#a = FeatureCountFast(subset = FILEARRAY)[0]
#dumpPklFile(a, "ftf12.pkl")
#FEATUREARRAY, SKIPDICT, fileToFeature, SUBSET = FeatureCount(subset = MKS.FILEARRAY, inputDirectory = "OpcodeList/")
#dumpPklFile(FEATUREARRAY, "NgramDict.pkl", 'trt/')
#dumpPklFile(SKIPDICT, "SkipDict.pkl", 'trt/')

#for i in SKIPDICT:
#    if len(SKIPDICT[i].keys())>1:
#        for ii in SKIPDICT[i]:
#            if SKIPDICT[i][ii]>150:
#                print(max(SKIPDICT[i]), ':', SKIPDICT[i][ii])
#                print(intToOpcode(max(SKIPDICT[i]), FEATUREARRAY, i))
#                print('')

#opcodes = loadPklFile(SUBSET[0],"OpcodeList/")
#a, dictOfNgram = getAllNgrams(3, opcodes)
#print("a1 = ", a[1])
#print("a2 = ", dictOfNgram[a[2]])
#print("a3 = ", dictOfNgram[a[3]])
#print("a4 = ", intToOpcode(a[4], dictOfNgram))
#b, dictOfSkipGram = getAllSkipGrams(3, a, dictOfNgram)
#print(len(b))
#print("b1 (with skip = 3) = ", dictOfSkipGram[b[1]])
#c, dictOfSkipGram = getAllSkipGramsInRange(4, a, dictOfSkipGram)
#print(len(c))
#print("c with distance 3 = ", skipToOpcode(d(c, 3)[1], dictOfSkipGram, dictOfNgram))
#f = getSkipDict(c, 4)
#print(len(f))


'''
    def prepareFeatureCountI(N = 4, maxDistance = 6, subset = None, subsetCount = 100, inputFolder = "OpcodeList/", powerOf2 = 18, Integer = False):
    toReturn, subset =  (False, subset) if subset is not None else (True, MKS.createSubset(subsetCount))
    fileToFeature = []
    featureDict = {}
    for filename in progressbar.ProgressBar()(subset):
    opcodes = loadPklFile(filename, inputFolder)
    sparseVector = HA.getHashFeatureArray(powerOf2 = powerOf2)
    NGramLists = getAllSkipGrams(maxDistance, getAllNgrams(N, opcodes))
    fileFeatures = countNGramLists(NGramLists, sparseVector, powerOf2)
    fileToFeature += fileFeatures
    featureDict = getSkipDict(NGramLists, powerOf2=18)
    if toReturn:
    return (subset, np.array(fileToFeature), featureDict)
    return (np.array(fileToFeature), featureDict)
    
    def mkFeature(listOfOpcodes):
    ''
    Function that makes a feature from a list of opcode (with spaces separating
    the opcodes)
    ''
    theFeature = ""
    for op in listOfOpcodes:
    theFeature += op + " "
    return theFeature[0:-1]
    
    def countFeaturesInRange(N, opcodeList, orderedCountList, powerOf2):
    ''
    Function that counts the number of times every feature apears in a list of
    opcodes. It takes as argument:
    - N : the number of opcodes in an N-Gram (for features)
    - opcodeList : a list of ordered opcodes
    - orderedCountList : a list containing all possible features in the right
    order to make a sparse vector.
    ''
    for i in range(1, N+1):
    for index in range(len(opcodeList) - i):
    orderedCountList[HA.getHashN(mkFeature(opcodeList[index:index+i]), powerOf2 = powerOf2)] +=1
    return sp.csr_matrix(orderedCountList)
    
    def prepareFeatureCount(subset = None, subsetCount = 100, inputFolder = "OpcodeList/", powerOf2 = 18, Integer = False):
    ''
    This function is made of all the functions above and deals with all the feature
    counting.
    It takes as arguments:
    - subset : a subset of files
    - subsetCount : if no subset was provided, the length of a desired subset (100 by default)
    - inputFolder: the folder where to find all the lists of opcodes
    - FeatureArray: an ordered list containing all the possible features.
    ''
    toReturn, subset =  (False, subset) if subset is not None else (True, MKS.createSubset(subsetCount))
    fileToFeature = []
    for filename in progressbar.ProgressBar()(subset):
    opcodes = loadPklFile(filename, inputFolder)
    sparseVector = HA.getHashFeatureArray(powerOf2)
    fileFeatures = countFeaturesInRange(3, opcodes, sparseVector, powerOf2 = powerOf2)
    fileToFeature += [fileFeatures]
    if toReturn:
    return (subset, np.array(fileToFeature))
    return np.array(fileToFeature)
    
    def countFeaturesInRangeI(N, opcodeList, orderedCountList, powerOf2):
    ''
    Function that counts the number of times every feature apears in a list of
    opcodes. It takes as argument:
    - N : the number of opcodes in an N-Gram (for features)
    - opcodeList : a list of ordered opcodes
    - orderedCountList : a list containing all possible features in the right
    order to make a sparse vector.
    ''
    for i in range(1, N+1):
    for index in range(len(opcodeList) - i):
    orderedCountList[HA.getHashN(mkFeatureI(opcodeList[index:index+i]), powerOf2 = powerOf2)] +=1
    return sp.csr_matrix(orderedCountList)
    
    ''
    use
    less
    ''
    def countNGramLists(NGramLists, sparseVector, powerOf2):
    ''
    This function counts the number of time a single Ngram is used in the list of Ngrams
    provided as argument. it then returns a sparse matrix made from the sparsevector
    made by the function.
    ''
    for l in NGramLists:
    for i in l:
    sparseVector[HA.getHashN(i, powerOf2)] +=1
    return sp.csr_matrix(sparseVector)
    
    
    ON THE ENTIRE SUBSET:
    Subset has been provided (default args)
    100% (11982 of 11982) |###################| Elapsed Time: 2:11:32 Time: 2:11:32
    '''
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code>\n",
    "# Fighting-malware-using-machine-learning\n",
    "\n",
    "To run this program, follow these steps:\n",
    "\n",
    "For this program to work, it needs the following files:\n",
    "    classificationAlgorithm.pkl  a trained Scikit-Learn classification algorithm\n",
    "    detectionAlgorithm.pkl       a trained Scikit-Learn classification algorithm\n",
    "    algorithmConfig.pkl          a file containing the config of the algorithms\n",
    "    \n",
    "It also requires the following libraries to be installed:\n",
    "    Capstone        pip install Capstone==3.0.5rc2\n",
    "                    http://www.capstone-engine.org/documentation.html\n",
    "                    http://www.capstone-engine.org/download.html          \n",
    "    Docopt          pip install Docopt==0.6.2\n",
    "    numpy           pip install numpy\n",
    "    scipy           pip install scipy\n",
    "    progressbar     pip install brogressbar2\n",
    "    Pickle          pip install pickle\n",
    "    pyparsing       pip install pyparsing\n",
    "    sklearn         conda install scikit-learn\n",
    "                    pip install -U scikit-learn    \n",
    "Usage:\n",
    "    malware_predict.py -i <file>\n",
    "    malware_predict.py -d <file>\n",
    "    malware_predict.py -b <file>\n",
    "    malware_predict.py -f -i <folder>\n",
    "    malware_predict.py -f -d <folder>\n",
    "    malware_predict.py -f -b <folder>\n",
    "Options:\n",
    "    -i      specify file name to be analysed with IDA Pro files\n",
    "    -d      specify file name to be analysed with files to be disassembled\n",
    "            disassembled\n",
    "    -b      specify file name to be analysed with text files containing binary\n",
    "    -f      specify folder name to be analysed\n",
    "    -h      show this screen\n",
    "\n",
    "Example of command:\n",
    "    user$ python malware_predict.py -f -i asm_files\n",
    "\n",
    "Both bytes_files and asm_files are folders containing sample data from the 'Microsoft Malware Classification Challenge'\n",
    "To get all the data from the challenge, download it here: https://www.kaggle.com/c/malware-classification/data\n",
    "\n",
    "This program can also be used on files that are not from the 'Microsoft Malware Classification Challenge'\n",
    "such as the helloworld.exe program included in this project.\n",
    " \n",
    "@author: Jacques-Antoine Portal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The following cells will describe the process to read an assembly file and get a list of corresponding opcodes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import makeSubset as MKS\n",
    "import getFileFamily as FF\n",
    "import hashArray as HA\n",
    "import progressbar as pb\n",
    "import numpy as np\n",
    "import featureCount as fc\n",
    "import readFileASM as rASM\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This functions reads the .asm files from the input folder and dumps  an array of all the opcodes found in that file as a pkl file in the  output folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (91 of 91) |#########################| Elapsed Time: 0:41:25 Time: 0:41:25\n"
     ]
    }
   ],
   "source": [
    "#Default variables\n",
    "inputFolder = 'sample'\n",
    "outputFolder ='sampleOpcodeList'\n",
    "#How to call the function.\n",
    "\n",
    "emptyFiles, OPCODES = rASM.readAsmFile(inputFolder, outputFolder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To function, this function calls parseAsm(), which is a function using pyparsing. <br>\n",
    "It looks multiple patterns of the form: <br><br>\n",
    "<code>LETTERS+:+NUMBER_List(NNUMBER)_Optional(NUMBER+?)_WORD</code> <br>\n",
    "ex: CODE:000040 05 B3 81 23 21? mov <br> \n",
    "where: Letters = 'CODE', NUMBER = '000040' List(NUMBER) = '05 B3 81 23', Optional(NUMBER+?) = '21?' and WORD = 'mov' <br><br>\n",
    "If it finds such a patter, it will add the WORD (the opcode) to a list of opcodes, and once all lines have been checked, the function will return the list of opcodes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Here a file is loaded, and its content is saved in the variable source.\n",
    "fileName = '0aSTGBVRXeJhx5OcpsgC'\n",
    "asmFile = open(os.path.join(\"sample\", fileName + \".asm\"), \"r\", encoding='ISO-8859-1')\n",
    "source = asmFile.readlines()\n",
    "opcodeDic = {}\n",
    "\n",
    "# FileOpcodes will contain an ordered list containing all the opcodes (as Integers), and opcodeDic will \n",
    "# contain a mapping from every opcode to an integer.\n",
    "# The reason opcodeDic is passed as an argument because usually other files will have been parsed already\n",
    "# and the goal is to have the same maping for every file.\n",
    "FileOpcodes, opcodeDic = rASM.parseAsm(source, opcodeDic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This pattern matching is not perfect, it does pick up words that are not opcodes. The reason why the pattern is not changed is because it would take way too long to run the parsing through all the files if the parsing had to be exact.<br>\n",
    "In its current form it took 6 days to run this on all the files, which is allready too long. <br>\n",
    "One way to limit the amount of false opcodes picked up by the parsing function was to make a check function. <br>\n",
    "This function checks that: <br>\n",
    "\n",
    "    - the opcode has no capital letter\n",
    "    - the opcode's length is between 2 and 5 (included)\n",
    "    - the opcode picked up is not a opcode used for data description (db, dd, dw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(This last check is to limit the length of each list of opcodes, as these can represent 80% op a file's opcode.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "False\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(rASM.isItAnOpcode('mov'))\n",
    "print(rASM.isItAnOpcode('push'))\n",
    "print(rASM.isItAnOpcode('someoddlongword'))\n",
    "print(rASM.isItAnOpcode('a'))\n",
    "print(rASM.isItAnOpcode('AbKdd'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The following cells will dscribe the process to count every kmer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is done using the Feature Count function from featureCount:\n",
    "It takes as arguments: \n",
    "\n",
    "        - maxN: the maximum size of every Ngram\n",
    "        - maxDistance: the maximum distance between Ngrams used for skipGrams.\n",
    "        - outputFolder: a folder in the current dictionary where to store the created files\n",
    "        - subsetCount: the number of items in a subset if no subset is provided\n",
    "        - inputDirectory: where the files / lists of opcodes can be found\n",
    "        - subset: a list of fileNames. (optional)\n",
    "        - powerOf2 (a number that is used in the creation of features to limit the size of every feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It returns 4 objects:\n",
    "\n",
    "        - NGRAMDICT : a dictionary containg all the mappings between hashed numbers and Ngrams\n",
    "        - SKIPGRAMDICT: a dictionary containg all the mappings between hashed numbers and Ngrams\n",
    "        - fileToFeature: A sparse matrix containg the features to train a machine algorithm.\n",
    "        - SUBSET: the subset of files used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No subset provided; creating one\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (10 of 10) |#########################| Elapsed Time: 0:00:01 Time: 0:00:01\n"
     ]
    }
   ],
   "source": [
    "#def FeatureCount(maxN = 4, maxDistance = 2, outputFolder = 'trt/', subsetCount = 75\n",
    "#                      , inputDirectory = \"OpcodeList/\", subset = None, powerOf2 = 18):\n",
    "\n",
    "# Here the function is called to count 1 the features of one file:\n",
    "NGRAMDICT, SKIPGRAMDICT, fileToFeature, SUBSET = fc.FeatureCount(subsetCount = 10, inputDirectory = \"OpcodeList/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The function uses the files obtained before with readASM. They are loaded as such"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file: 01kcPWA9K2BOxQeS5Rju has : 557 opcodes\n",
      "Its first 10 opcodes are: [4, 4, 3, 4, 3, 10, 3, 4, 4, 17]\n"
     ]
    }
   ],
   "source": [
    "nameOfAFile = SUBSET[0]\n",
    "whereToFindIt = \"OpcodeList/\"\n",
    "opcodes = fc.loadPklFile(nameOfAFile, whereToFindIt)\n",
    "print('The file:', nameOfAFile, 'has :', len(opcodes), 'opcodes')\n",
    "print('Its first 10 opcodes are:', opcodes[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every opcode (integer) can be mapped back to its string form using the OPCODES dictionary (constant of the file, is loaded from a pickle file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Its first 20 opcodes are: ['push', 'push', 'mov', 'push', 'mov', 'sub', 'mov', 'push', 'push', 'lea']\n"
     ]
    }
   ],
   "source": [
    "print('Its first 20 opcodes are:', [fc.OPCODES[op] for op in opcodes[0:10]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Once having loaded a file, the FeatureCount function gets a list of all the Ngrams (in order) usign the getAllNgrams function.\n",
    "This function retruns a list of list of Ngrams, every list is a list of Ngram with a specific size. <br>\n",
    "listOfNgram = [listOf1Gram, listOf2gram, ..., listOfNgram]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ListOfNgram's second Ngram (size 3) is:  304256\n"
     ]
    }
   ],
   "source": [
    "ListOfNgram, dictOfNgram = fc.getAllNgrams(3, opcodes)\n",
    "print(\"ListOfNgram's second Ngram (size 3) is: \", ListOfNgram[2][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every hahsed Ngram can be mapped back to its orriginal Ngram using the returned dictOfNgram:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ListOfNgram's second Ngram (size 3) is:  (4, 3, 4)\n",
      "ListOfNgram's third Ngram (size 3) is:  (3, 4, 3)\n"
     ]
    }
   ],
   "source": [
    "print(\"ListOfNgram's second Ngram (size 3) is: \", list(dictOfNgram[ListOfNgram[2][1]].keys())[0])\n",
    "print(\"ListOfNgram's third Ngram (size 3) is: \", list(dictOfNgram[ListOfNgram[2][2]].keys())[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function has been made to get the mapping back in a easier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ListOfNgram's second Ngram (size 3) is:  ('push', 'mov', 'push')\n",
      "ListOfNgram's third Ngram (size 3) is:  ('mov', 'push', 'mov')\n"
     ]
    }
   ],
   "source": [
    "print(\"ListOfNgram's second Ngram (size 3) is: \", fc.intToOpcode(ListOfNgram[2][1], dictOfNgram))\n",
    "print(\"ListOfNgram's third Ngram (size 3) is: \", fc.intToOpcode(ListOfNgram[2][2], dictOfNgram))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then there is a function to map a list of dictioanries from countNGramDict to a sparse matrix:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make subsets and get file's classes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section will explain how subsets are created, and how to obtain the classes corresponding to every file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a csv file containing a list of all the files and their corresponding families.<br>\n",
    "importing the file getFileFamily.py will create a dictionary using the csv file, and using the function getFileFamily, it is possible to obtain the class of a file.<br>\n",
    "The cell bellow will show how this can be done for a subset of files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 2, 3, 3, 5, 6, 7, 7, 8]\n"
     ]
    }
   ],
   "source": [
    "FILEINDEXES = FF.getFilesIndex(SUBSET)\n",
    "FILEFAMILIES = [FF.getFamily(i) for i in FILEINDEXES]\n",
    "print(FILEFAMILIES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a subset with at least one file per class, the subset must contain at least 75 files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make a subset of file, the function createSubset is used (from makeSubset). <br>\n",
    "It takes as argument the number of files desied in the subset, the the class of each file (if different to the default one), the name of each file and the family repartition (how many files per family are there in the training data set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of subset before: 10\n",
      "length of subset after: 75\n"
     ]
    }
   ],
   "source": [
    "#def createSubset(subsetSize, fileFamily = FILEFAMILY, fileArray = FILEARRAY, familyRepartition = FAMILYREPARTITION):\n",
    "print('length of subset before:' ,len(SUBSET))\n",
    "SUBSET = MKS.createSubset(75)\n",
    "print('length of subset after:' ,len(SUBSET))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Scikit learn, classification can be done in the following way:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell splits the data into 2 test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 2048)\n",
      "(75, 2048)\n"
     ]
    }
   ],
   "source": [
    "from numpy.random import permutation\n",
    "from sklearn.preprocessing import normalize\n",
    "fileToFeature = normalize(fileToFeature, 'l1')\n",
    "\n",
    "fileToFeature, SUBSET = fc.getFeatureMatrix(3, 3, subsetCount = 100, inputDirectory = \"OpcodeList/\", powerOf2 = 10)\n",
    "print(fileToFeature.shape)\n",
    "\n",
    "FILEINDEXES = FF.getFilesIndex(SUBSET)\n",
    "FILEFAMILIES = np.array([FF.getFamily(i) for i in FILEINDEXES]) #obtaining the targets\n",
    "\n",
    "N = fileToFeature.shape[0]\n",
    "I = permutation(N)   # Shuffled indices 0,..., N-1\n",
    "Itr = I[:75]\n",
    "Ite = I[75:]\n",
    "\n",
    "Xtr = fileToFeature[Itr,:]\n",
    "ttr = FILEFAMILIES[Itr]\n",
    "\n",
    "Xte = fileToFeature[Ite,:]\n",
    "tte = FILEFAMILIES[Ite]\n",
    "print(Xtr.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be done with any machine learning algorithm from scikit learn.\n",
    "To then save the faile, the dumpPklFile function can be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.72\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "clf = ExtraTreesClassifier(n_estimators=200, random_state=0)\n",
    "clf.fit(Xtr, ttr)\n",
    "prediction = clf.predict(Xte)\n",
    "compare = prediction == tte\n",
    "print(np.mean(compare))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}


# Fighting-malware-using-machine-learning
<pre>
To run this program, follow these steps:

For this program to work, it needs the following files:
    classificationAlgorithm.pkl  a trained Scikit-Learn classification algorithm
    detectionAlgorithm.pkl       a trained Scikit-Learn classification algorithm
    algorithmConfig.pkl          a file containing the config of the algorithms
    
It also requires the following libraries to be installed:
    Capstone        pip install Capstone==3.0.5rc2
                    http://www.capstone-engine.org/documentation.html
                    http://www.capstone-engine.org/download.html          
    Docopt          pip install Docopt==0.6.2
    numpy           pip install numpy
    scipy           pip install scipy
    progressbar     pip install brogressbar2
    Pickle          pip install pickle
    pyparsing       pip install pyparsing
    sklearn         conda install scikit-learn
                    pip install -U scikit-learn    
Usage:
    malware_predict.py -i <file>
    malware_predict.py -d <file>
    malware_predict.py -b <file>
    malware_predict.py -f -i <folder>
    malware_predict.py -f -d <folder>
    malware_predict.py -f -b <folder>
Options:
    -i      specify file name to be analysed with IDA Pro files
    -d      specify file name to be analysed with files to be disassembled
            disassembled
    -b      specify file name to be analysed with text files containing binary
    -f      specify folder name to be analysed
    -h      show this screen

Example of command:
    user$ python malware_predict.py -f -i asm_files

Both bytes_files and asm_files are folders containing sample data from the 'Microsoft Malware Classification Challenge'
To get all the data from the challenge, download it here: https://www.kaggle.com/c/malware-classification/data

This program can also be used on files that are not from the 'Microsoft Malware Classification Challenge'
such as the helloworld.exe program included in this project.
 
@author: Jacques-Antoine Portal

</pre>

# The following cells will describe the process to read an assembly file and get a list of corresponding opcodes:


```python
import makeSubset as MKS
import getFileFamily as FF
import hashArray as HA
import progressbar as pb
import numpy as np
import featureCount as fc
import readFileASM as rASM
import pickle
import os
```

### This functions reads the .asm files from the input folder and dumps  an array of all the opcodes found in that file as a pkl file in the  output folder.


```python
#Default variables
inputFolder = 'sample'
outputFolder ='sampleOpcodeList'
#How to call the function.

emptyFiles, OPCODES = rASM.readAsmFile(inputFolder, outputFolder)
```

    100% (91 of 91) |#########################| Elapsed Time: 0:41:25 Time: 0:41:25
    

To function, this function calls parseAsm(), which is a function using pyparsing. <br>
It looks multiple patterns of the form: <br><br>
<code>LETTERS+:+NUMBER_List(NNUMBER)_Optional(NUMBER+?)_WORD</code> <br>
ex: CODE:000040 05 B3 81 23 21? mov <br> 
where: Letters = 'CODE', NUMBER = '000040' List(NUMBER) = '05 B3 81 23', Optional(NUMBER+?) = '21?' and WORD = 'mov' <br><br>
If it finds such a patter, it will add the WORD (the opcode) to a list of opcodes, and once all lines have been checked, the function will return the list of opcodes.



```python
#Here a file is loaded, and its content is saved in the variable source.
fileName = '0aSTGBVRXeJhx5OcpsgC'
asmFile = open(os.path.join("sample", fileName + ".asm"), "r", encoding='ISO-8859-1')
source = asmFile.readlines()
opcodeDic = {}

# FileOpcodes will contain an ordered list containing all the opcodes (as Integers), and opcodeDic will 
# contain a mapping from every opcode to an integer.
# The reason opcodeDic is passed as an argument because usually other files will have been parsed already
# and the goal is to have the same maping for every file.
FileOpcodes, opcodeDic = rASM.parseAsm(source, opcodeDic)
```

This pattern matching is not perfect, it does pick up words that are not opcodes. The reason why the pattern is not changed is because it would take way too long to run the parsing through all the files if the parsing had to be exact.<br>
In its current form it took 6 days to run this on all the files, which is allready too long. <br>
One way to limit the amount of false opcodes picked up by the parsing function was to make a check function. <br>
This function checks that: <br>

    - the opcode has no capital letter
    - the opcode's length is between 2 and 5 (included)
    - the opcode picked up is not a opcode used for data description (db, dd, dw)

(This last check is to limit the length of each list of opcodes, as these can represent 80% op a file's opcode.)


```python
print(rASM.isItAnOpcode('mov'))
print(rASM.isItAnOpcode('push'))
print(rASM.isItAnOpcode('someoddlongword'))
print(rASM.isItAnOpcode('a'))
print(rASM.isItAnOpcode('AbKdd'))
```

    True
    True
    False
    False
    False
    

# The following cells will dscribe the process to count every kmer:

This is done using the Feature Count function from featureCount:
It takes as arguments: 

        - maxN: the maximum size of every Ngram
        - maxDistance: the maximum distance between Ngrams used for skipGrams.
        - outputFolder: a folder in the current dictionary where to store the created files
        - subsetCount: the number of items in a subset if no subset is provided
        - inputDirectory: where the files / lists of opcodes can be found
        - subset: a list of fileNames. (optional)
        - powerOf2 (a number that is used in the creation of features to limit the size of every feature.

It returns 4 objects:

        - NGRAMDICT : a dictionary containg all the mappings between hashed numbers and Ngrams
        - SKIPGRAMDICT: a dictionary containg all the mappings between hashed numbers and Ngrams
        - fileToFeature: A sparse matrix containg the features to train a machine algorithm.
        - SUBSET: the subset of files used.


```python
#def FeatureCount(maxN = 4, maxDistance = 2, outputFolder = 'trt/', subsetCount = 75
#                      , inputDirectory = "OpcodeList/", subset = None, powerOf2 = 18):

# Here the function is called to count 1 the features of one file:
NGRAMDICT, SKIPGRAMDICT, fileToFeature, SUBSET = fc.FeatureCount(subsetCount = 10, inputDirectory = "OpcodeList/")
```

    No subset provided; creating one
    

    100% (10 of 10) |#########################| Elapsed Time: 0:00:01 Time: 0:00:01
    

### The function uses the files obtained before with readASM. They are loaded as such


```python
nameOfAFile = SUBSET[0]
whereToFindIt = "OpcodeList/"
opcodes = fc.loadPklFile(nameOfAFile, whereToFindIt)
print('The file:', nameOfAFile, 'has :', len(opcodes), 'opcodes')
print('Its first 10 opcodes are:', opcodes[0:10])
```

    The file: 01kcPWA9K2BOxQeS5Rju has : 557 opcodes
    Its first 10 opcodes are: [4, 4, 3, 4, 3, 10, 3, 4, 4, 17]
    

Every opcode (integer) can be mapped back to its string form using the OPCODES dictionary (constant of the file, is loaded from a pickle file)


```python
print('Its first 10 opcodes are:', [fc.OPCODES[op] for op in opcodes[0:10]])
```

    Its first 10 opcodes are: ['push', 'push', 'mov', 'push', 'mov', 'sub', 'mov', 'push', 'push', 'lea']
    

### Once having loaded a file, the FeatureCount function gets a list of all the Ngrams (in order) usign the getAllNgrams function.
This function retruns a list of list of Ngrams, every list is a list of Ngram with a specific size. <br>
listOfNgram = [listOf1Gram, listOf2gram, ..., listOfNgram]


```python
ListOfNgram, dictOfNgram = fc.getAllNgrams(3, opcodes)
print("ListOfNgram's second Ngram (size 3) is: ", ListOfNgram[2][1])
```

    ListOfNgram's second Ngram (size 3) is:  304256
    

Every hahsed Ngram can be mapped back to its orriginal Ngram using the returned dictOfNgram:


```python
print("ListOfNgram's second Ngram (size 3) is: ", list(dictOfNgram[ListOfNgram[2][1]].keys())[0])
print("ListOfNgram's third Ngram (size 3) is: ", list(dictOfNgram[ListOfNgram[2][2]].keys())[0])
```

    ListOfNgram's second Ngram (size 3) is:  (4, 3, 4)
    ListOfNgram's third Ngram (size 3) is:  (3, 4, 3)
    

A function has been made to get the mapping back in a easier:


```python
print("ListOfNgram's second Ngram (size 3) is: ", fc.intToOpcode(ListOfNgram[2][1], dictOfNgram))
print("ListOfNgram's third Ngram (size 3) is: ", fc.intToOpcode(ListOfNgram[2][2], dictOfNgram))
```

    ListOfNgram's second Ngram (size 3) is:  ('push', 'mov', 'push')
    ListOfNgram's third Ngram (size 3) is:  ('mov', 'push', 'mov')
    

Then there is a function to map a list of dictioanries from countNGramDict to a sparse matrix:

# Make subsets and get file's classes:

This section will explain how subsets are created, and how to obtain the classes corresponding to every file.

There is a csv file containing a list of all the files and their corresponding families.<br>
importing the file getFileFamily.py will create a dictionary using the csv file, and using the function getFileFamily, it is possible to obtain the class of a file.<br>
The cell bellow will show how this can be done for a subset of files:


```python
FILEINDEXES = FF.getFilesIndex(SUBSET)
FILEFAMILIES = [FF.getFamily(i) for i in FILEINDEXES]
print(FILEFAMILIES)
```

    [1, 2, 2, 3, 3, 5, 6, 7, 7, 8]
    

To get a subset with at least one file per class, the subset must contain at least 75 files.

To make a subset of file, the function createSubset is used (from makeSubset). <br>
It takes as argument the number of files desied in the subset, the the class of each file (if different to the default one), the name of each file and the family repartition (how many files per family are there in the training data set).


```python
#def createSubset(subsetSize, fileFamily = FILEFAMILY, fileArray = FILEARRAY, familyRepartition = FAMILYREPARTITION):
print('length of subset before:' ,len(SUBSET))
SUBSET = MKS.createSubset(75)
print('length of subset after:' ,len(SUBSET))
```

    length of subset before: 10
    length of subset after: 75
    

 # classification

Using Scikit learn, classification can be done in the following way:

This cell splits the data into 2 test sets


```python
from numpy.random import permutation
from sklearn.preprocessing import normalize
fileToFeature = normalize(fileToFeature, 'l1')

fileToFeature, SUBSET = fc.getFeatureMatrix(3, 3, subsetCount = 100, inputDirectory = "OpcodeList/", powerOf2 = 10)
print(fileToFeature.shape)

FILEINDEXES = FF.getFilesIndex(SUBSET)
FILEFAMILIES = np.array([FF.getFamily(i) for i in FILEINDEXES]) #obtaining the targets

N = fileToFeature.shape[0]
I = permutation(N)   # Shuffled indices 0,..., N-1
Itr = I[:75]
Ite = I[75:]

Xtr = fileToFeature[Itr,:]
ttr = FILEFAMILIES[Itr]

Xte = fileToFeature[Ite,:]
tte = FILEFAMILIES[Ite]
print(Xtr.shape)
```

    (100, 2048)
    (75, 2048)
    

This can be done with any machine learning algorithm from scikit learn.
To then save the faile, the dumpPklFile function can be used.


```python
from sklearn.ensemble import ExtraTreesClassifier
clf = ExtraTreesClassifier(n_estimators=200, random_state=0)
clf.fit(Xtr, ttr)
prediction = clf.predict(Xte)
compare = prediction == tte
print(np.mean(compare))
```

    0.72
    
